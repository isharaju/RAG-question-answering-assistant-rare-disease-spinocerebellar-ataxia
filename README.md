# Retrieval-Augmented Generation (RAG) Pipeline for Question Answering Assistant for Rare Disease Spinocerebellar Ataxia

This repository implements a modular Retrieval-Augmented Generation (RAG) system designed for answering domain-specific questions using scientific PDF documents, with a focus on rare diseases like **Spinocerebellar Ataxia (SCA)**.

---

## ğŸ“ Project Structure
<pre> 
a03/
â”œâ”€â”€ hu_sp25_691_a03-3/ # Main pipeline and source code
â”‚ â”œâ”€â”€ classes/ # Core pipeline components
â”‚ â”‚ â”œâ”€â”€ chromadb_retriever.py # Vector search logic
â”‚ â”‚ â”œâ”€â”€ config_manager.py # Loads config from JSON
â”‚ â”‚ â”œâ”€â”€ document_ingestor.py # Extracts text from PDFs
â”‚ â”‚ â”œâ”€â”€ embedding_loader.py # Loads stored embeddings
â”‚ â”‚ â”œâ”€â”€ embedding_preparer.py # Generates embeddings
â”‚ â”‚ â”œâ”€â”€ llm_client.py # Sends prompts to local LLM
â”‚ â”‚ â”œâ”€â”€ rag_query_processor.py # Full RAG orchestration
â”‚ â”‚ â””â”€â”€ utilities.py # Common helpers
â”‚ â”‚
â”‚ â”œâ”€â”€ data/ # Input/output storage
â”‚ â”‚ â”œâ”€â”€ original_pdf_documents/ # Source PDFs
â”‚ â”‚ â”œâ”€â”€ raw_input/ # Filenames to ingest
â”‚ â”‚ â”œâ”€â”€ cleaned_text/ # Cleaned + chunked text files
â”‚ â”‚ â”œâ”€â”€ embeddings/ # Chunk embeddings in JSON
â”‚ â”‚ â”œâ”€â”€ vectordb/ # ChromaDB local DB
â”‚ â”‚
â”‚ â”œâ”€â”€ scripts/ # Bash automation scripts
â”‚ â”œâ”€â”€ tests/ # Unit tests
â”‚ â”œâ”€â”€ main.py # Entrypoint for pipeline steps
â”‚ â”œâ”€â”€ llm_server.py # Starts local LLM server
â”‚ â”œâ”€â”€ config.json # Main config file
â”‚ â””â”€â”€ requirements.txt # Python dependencies
â”‚
â”œâ”€â”€ models/ # Local LLM model files
â”‚ â””â”€â”€ mistral-7b-instruct-v0.1.*.gguf
</pre>
---

## âš™ï¸ Setup Instructions

1. **Clone the repo**:
<pre> 
```bash
git clone https://github.com/your-username/hu_sp25_691_a03-3.git
cd hu_sp25_691_a03-3
</pre>

2. Create virtual environment:
<pre> 
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt
</pre>

3. Download LLM locally (e.g., Mistral 7B GGUF)
4. Start the LLM API:
<pre> 
python llm_server.py
</pre>
5. Run the pipeline:
<pre> 
./scripts/run_pipeline_steps.sh
</pre>

ğŸ§ª Example Query

â“ "What are some biomarkers of spinocerebellar ataxia?"
The pipeline:

Extracts text from PDFs
Chunks & embeds them
Retrieves top relevant chunks
Feeds them into Mistral-7B for grounded response
âœ… Features

Modular & testable class-based structure
Local LLM integration via API
Uses SentenceTransformers for embedding
Retrieval via ChromaDB vector store
Supports RAG and non-RAG response modes
